{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "TRAIN_FILE = 'train_emoji_cleaned.csv'\n",
    "TEST_FILE = 'test_emoji_cleaned.csv'\n",
    "train_voc = ''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def tokenize(lines, token='word'):\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print(\"error: unknown type\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def count_corpus(tokens):\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "\n",
    "        counter = count_corpus(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens\n",
    "\n",
    "        uniq_tokens += [\n",
    "            token for token, freq in self.token_freqs\n",
    "            if freq >= min_freq and token not in uniq_tokens]\n",
    "        self.idx_to_token, self.token_to_idx = [], dict()\n",
    "        for token in uniq_tokens:\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "global train_voc\n",
    "train_raw_data = pd.read_csv(TRAIN_FILE)\n",
    "train_text_list = list()\n",
    "\n",
    "for single_text in train_raw_data.Tweet:\n",
    "    single_text = re.sub(r'[\\W]+', ' ', single_text.lower())\n",
    "    single_text = single_text.replace('hadn t', 'had not')\n",
    "    single_text = single_text.replace('wasn t', 'was not')\n",
    "    single_text = single_text.replace('didn t', 'did not')\n",
    "    single_text = single_text.replace('havent', 'have not')\n",
    "    single_text = single_text.replace('isn t', 'is not')\n",
    "    single_text = single_text.replace('isnt', 'is not')\n",
    "    single_text = single_text.replace('doesnt', 'does not')\n",
    "    single_text = single_text.replace('dont', 'do not')\n",
    "    single_text = single_text.replace('hasnt', 'has not')\n",
    "    single_text = single_text.replace('arent', 'are not')\n",
    "    single_text = single_text.replace('•', '')\n",
    "    single_text = single_text.replace('••', '')\n",
    "    single_text = single_text.replace('  ', ' ')\n",
    "    single_text = single_text.replace('   ', ' ')\n",
    "\n",
    "    train_text_list.append(single_text)\n",
    "\n",
    "train_tokens = tokenize(train_text_list)\n",
    "train_voc = Vocab(tokens=train_tokens, min_freq=50)\n",
    "\n",
    "list_init = list()\n",
    "for i in range(len(train_raw_data)):\n",
    "    list_init.append(np.zeros(len(train_voc) - 1))\n",
    "\n",
    "train_cleaned = train_raw_data\n",
    "train_cleaned['tokens'] = list_init\n",
    "\n",
    "list_init.clear()\n",
    "for i in range(len(train_raw_data)):\n",
    "    list_init.append(np.zeros(20))\n",
    "\n",
    "train_cleaned['emo_vecs'] = list_init\n",
    "\n",
    "clean_tokens = np.zeros((len(train_raw_data), len(train_voc) - 1))\n",
    "clean_emos = np.zeros((len(train_raw_data), 20))\n",
    "\n",
    "for i in range(len(train_cleaned)):\n",
    "    cur_text = train_cleaned.iloc[i]['Tweet']\n",
    "    cur_words = cur_text.split()\n",
    "    cur_indices = np.zeros(len(train_voc) - 1)\n",
    "\n",
    "    for word in cur_words:\n",
    "        temp_index = train_voc[word]\n",
    "        if temp_index == 0:\n",
    "            continue\n",
    "        cur_indices[temp_index - 1] += 1\n",
    "\n",
    "    train_cleaned.at[i, 'tokens'] = cur_indices\n",
    "    clean_tokens[i] = cur_indices\n",
    "\n",
    "    cur_emos = np.zeros(20)\n",
    "    cur_emos[train_cleaned.iloc[i]['Label']] = 1\n",
    "\n",
    "    train_cleaned.at[i, 'emo_vecs'] = cur_emos\n",
    "    clean_emos[i] = cur_emos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(                                                   Tweet  Label  \\\n 0      a little throwback with my favourite person  w...      0   \n 1      glam on  yesterday for kcon makeup using  in f...      7   \n 2      democracy plaza in the wake of a stunning outc...     11   \n 3           then amp now vilo  walt disney magic kingdom      0   \n 4                       who never  a galaxy far far away      2   \n ...                                                  ...    ...   \n 39995  date night  outback steakhouse  birmingham  fu...      0   \n 39996  just taking some time out of my day to say i a...      0   \n 39997                     parenthood binge watch all day      5   \n 39998  saturday morning building project futurehandym...      8   \n 39999  ️love last nights adventure to lookout mountai...      0   \n \n                                                   tokens  \\\n 0      [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...   \n 1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...   \n 2      [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, ...   \n 3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n 4      [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...   \n ...                                                  ...   \n 39995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n 39996  [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n 39997  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n 39998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n 39999  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n \n                                                 emo_vecs  \n 0      [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n 1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n 2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n 3      [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n 4      [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n ...                                                  ...  \n 39995  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n 39996  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n 39997  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n 39998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n 39999  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n \n [40000 rows x 4 columns],\n array([[0., 1., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [1., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 1., ..., 0., 0., 0.]]),\n array([[1., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [1., 0., 0., ..., 0., 0., 0.]]))"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cleaned, clean_tokens, clean_emos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<unk>': 0, 'the': 1, 'my': 2, 'to': 3, 'i': 4, 'a': 5, 'you': 6, 'in': 7, 'and': 8, 'of': 9, 'with': 10, 'for': 11, 'this': 12, 'love': 13, 'is': 14, 'at': 15, 'new': 16, 'on': 17, 'so': 18, 'me': 19, 'it': 20, 'happy': 21, 'day': 22, 'amp': 23, 'we': 24, 'be': 25, 'not': 26, 'was': 27, 'all': 28, 'im': 29, 'that': 30, 'from': 31, 'your': 32, 'beach': 33, 'its': 34, 'night': 35, 'do': 36, 'just': 37, 'have': 38, 'one': 39, 'best': 40, 'york': 41, 'out': 42, 'when': 43, 'park': 44, 'are': 45, 'good': 46, 'today': 47, 'time': 48, 'our': 49, 'but': 50, 'up': 51, 'by': 52, 'university': 53, 'christmas': 54, 'like': 55, 'birthday': 56, 'these': 57, 'city': 58, 'last': 59, 'got': 60, 'little': 61, 'thanks': 62, 'beautiful': 63, 'get': 64, 'great': 65, 'what': 66, 'california': 67, 'see': 68, 'back': 69, 'center': 70, 'family': 71, 'life': 72, 'school': 73, 'thank': 74, 'favorite': 75, 'some': 76, 'had': 77, 'much': 78, 'fun': 79, 'cant': 80, 'home': 81, 'always': 82, 'first': 83, 'her': 84, 'here': 85, 'high': 86, 'amazing': 87, 'state': 88, 'go': 89, 'girl': 90, 'an': 91, 'tonight': 92, 'no': 93, 'friends': 94, 'us': 95, 'as': 96, 'more': 97, 'morning': 98, 'now': 99, 'lake': 100, 'nyc': 101, 'florida': 102, 'how': 103, 'know': 104, 'texas': 105, 'weekend': 106, 'year': 107, 'who': 108, 'friend': 109, 'people': 110, 'can': 111, 'about': 112, 'look': 113, 'place': 114, 'world': 115, 'come': 116, 'if': 117, 'were': 118, 'san': 119, 'he': 120, 'house': 121, 'miami': 122, 'she': 123, 'only': 124, 'baby': 125, 'too': 126, 'two': 127, 'still': 128, 'way': 129, 'south': 130, 'th': 131, 'w': 132, 'will': 133, 'work': 134, 'they': 135, 'off': 136, 'st': 137, 'miss': 138, 'make': 139, 'show': 140, 'girls': 141, 'lol': 142, 'been': 143, 'big': 144, 'never': 145, 'chicago': 146, 'take': 147, 'north': 148, 'right': 149, 'los': 150, 'ever': 151, 'his': 152, 'la': 153, 'u': 154, 'another': 155, 'days': 156, 'being': 157, 'youre': 158, 'am': 159, 'bar': 160, 'heart': 161, 'stadium': 162, 'summer': 163, 'wait': 164, 'college': 165, 'downtown': 166, 'tbt': 167, 'club': 168, 'better': 169, 'angeles': 170, 'getting': 171, 'has': 172, 'there': 173, 'made': 174, 'lit': 175, 'vegas': 176, 'thankful': 177, 'date': 178, 'because': 179, 'or': 180, 'than': 181, 'music': 182, 'man': 183, 'dinner': 184, 'them': 185, 'even': 186, 'pretty': 187, 'years': 188, 'week': 189, 'disney': 190, 'ready': 191, 'photo': 192, 'again': 193, 'after': 194, 'party': 195, 'square': 196, 'view': 197, 'going': 198, 'old': 199, 'game': 200, 'such': 201, 'island': 202, 'carolina': 203, 'art': 204, 'over': 205, 'next': 206, 'him': 207, 'perfect': 208, 'repost': 209, 'ny': 210, 'live': 211, 'finally': 212, 'merry': 213, 'really': 214, 'washington': 215, 'west': 216, 'sunday': 217, 'times': 218, 'guys': 219, 'down': 220, 'hollywood': 221, 'ohio': 222, 'oh': 223, 'picture': 224, 'say': 225, 'hair': 226, 'need': 227, 'most': 228, 'brooklyn': 229, 'want': 230, 'guy': 231, 'selfie': 232, 'magic': 233, 'las': 234, 'hotel': 235, 'church': 236, 'before': 237, 'think': 238, 'yall': 239, 'thats': 240, 'friday': 241, 'did': 242, 'coming': 243, 'well': 244, 'usa': 245, 'proud': 246, 'sweet': 247, 'ive': 248, 'hope': 249, 'georgia': 250, 'every': 251, 'long': 252, 'hall': 253, 'everyone': 254, 'cute': 255, 'mom': 256, 'national': 257, 'could': 258, 'coffee': 259, 'street': 260, 's': 261, 'restaurant': 262, 'glad': 263, 'very': 264, 'atlanta': 265, 'airport': 266, 'awesome': 267, 'boy': 268, 'where': 269, 'fire': 270, 'international': 271, 'things': 272, 'hot': 273, 'pic': 274, 'saturday': 275, 'real': 276, 'resort': 277, 'missing': 278, 'ill': 279, 'ya': 280, 'wedding': 281, 'would': 282, 'lunch': 283, 'came': 284, 'ca': 285, 'into': 286, 'having': 287, 'tree': 288, 'red': 289, 'god': 290, 'museum': 291, 'shes': 292, 'blue': 293, 'while': 294, 'michigan': 295, 'food': 296, 'check': 297, 'making': 298, 'houston': 299, 'soon': 300, 'dance': 301, 'looking': 302, 'other': 303, 'rock': 304, 'stop': 305, 'everything': 306, 'tomorrow': 307, 'ladies': 308, 'blessed': 309, 'season': 310, 'around': 311, 'field': 312, 'toronto': 313, 'holiday': 314, 'grand': 315, 'town': 316, 'why': 317, 'crazy': 318, 'white': 319, 'does': 320, 'together': 321, 'fl': 322, 'nice': 323, 'let': 324, 'disneys': 325, 'central': 326, 'keep': 327, 'kingdom': 328, 'post': 329, 'excited': 330, 'fam': 331, 'lets': 332, 'nothing': 333, 'yesterday': 334, 'hills': 335, 'sunset': 336, 'thing': 337, 'shit': 338, 'boston': 339, 'sun': 340, 'found': 341, 'sister': 342, 'orlando': 343, 'fort': 344, 'forever': 345, 'their': 346, 'smile': 347, 'fall': 348, 'wonderful': 349, 'n': 350, 'boys': 351, 'makes': 352, 'cafe': 353, 'dallas': 354, 'district': 355, 'grill': 356, 'illinois': 357, 'garden': 358, 'virginia': 359, 'full': 360, 'face': 361, 'room': 362, 'feel': 363, 'lovely': 364, 'many': 365, 'country': 366, 'gonna': 367, 'cool': 368, 'trip': 369, 'america': 370, 'tennessee': 371, 'thanksgiving': 372, 'sure': 373, 'beauty': 374, 'yes': 375, 'bridge': 376, 'disneyland': 377, 'may': 378, 'doing': 379, 'lil': 380, 'breakfast': 381, 'spring': 382, 'meet': 383, 'bae': 384, 'took': 385, 'hard': 386, 'austin': 387, 'start': 388, 'county': 389, 'monday': 390, 'black': 391, 'lounge': 392, 'top': 393, 'bad': 394, 'already': 395, 'east': 396, 'call': 397, 'side': 398, 'mountain': 399, 'without': 400, 'special': 401, 'nashville': 402, 'jersey': 403, 'video': 404, 'gorgeous': 405, 'fitness': 406, 'santa': 407, 'hes': 408, 'studio': 409, 'manhattan': 410, 'tx': 411, 'loves': 412, 'couldnt': 413, 'shop': 414, 'team': 415, 'bay': 416, 'river': 417, 'loved': 418, 'welcome': 419, 'didnt': 420, 'kids': 421, 'point': 422, 'dad': 423, 'francisco': 424, 'super': 425, 'e': 426, 'taking': 427, 'put': 428, 'said': 429, 'newyork': 430, 'village': 431, 'weather': 432, 'hey': 433, 'aint': 434, 'studios': 435, 'few': 436, 'lot': 437, 'arena': 438, 'gotta': 439, 'brother': 440, 'dc': 441, 'moment': 442, 'stay': 443, 'end': 444, 'squad': 445, 'babe': 446, 'worth': 447, 'though': 448, 'since': 449, 'ocean': 450, 'celebrating': 451, 'arts': 452, 'halloween': 453, 'person': 454, 'through': 455, 'went': 456, 'someone': 457, 'looks': 458, 'theatre': 459, 'ontario': 460, 'nights': 461, 'those': 462, 'something': 463, 'true': 464, 'hill': 465, 'zoo': 466, 'free': 467, 'building': 468, 'link': 469, 'lights': 470, 'whole': 471, 'ice': 472, 'spend': 473, 'adventure': 474, 'wanna': 475, 'then': 476, 'falls': 477, 'loving': 478, 'throwback': 479, 'walk': 480, 'seattle': 481, 'id': 482, 'seeing': 483, 'mall': 484, 'shopping': 485, 'believe': 486, 'ass': 487, 'until': 488, 'shot': 489, 'set': 490, 'part': 491, 'concert': 492, 'class': 493, 'massachusetts': 494, 'tell': 495, 'guess': 496, 'arizona': 497, 'feeling': 498, 'sisters': 499, 'visit': 500, 'wcw': 501, 'almost': 502, 'cause': 503, 'play': 504, 'memorial': 505, 'early': 506, 'fashion': 507, 'please': 508, 'hello': 509, 'watch': 510, 'diego': 511, 'give': 512, 'x': 513, 'k': 514, 'break': 515, 'waiting': 516, 'pm': 517, 'done': 518, 'market': 519, 'brunch': 520, 'find': 521, 'light': 522, 'enjoying': 523, 'same': 524, 'bff': 525, 'pennsylvania': 526, 'sometimes': 527, 'starbucks': 528, 'missed': 529, 'senior': 530, 'holidays': 531, 'enjoy': 532, 'any': 533, 'tho': 534, 'else': 535, 'walt': 536, 'fav': 537, 'bday': 538, 'late': 539, 'valley': 540, 'springs': 541, 'follow': 542, 'bro': 543, 'kentucky': 544, 'photography': 545, 'event': 546, 'lady': 547, 'thought': 548, 'pier': 549, 'remember': 550, 't': 551, 'watching': 552, 'mcm': 553, 'should': 554, 'bio': 555, 'myself': 556, 'green': 557, 'also': 558, 'lucky': 559, 'run': 560, 'indiana': 561, 'heres': 562, 'american': 563, 'shoot': 564, 'months': 565, 'repostapp': 566, 'united': 567, 'store': 568, 'theres': 569, 'vote': 570, 'maryland': 571, 'orleans': 572, 'congrats': 573, 'creek': 574, 'hit': 575, 'three': 576, 'head': 577, 'alabama': 578, 'playing': 579, 'travel': 580, 'philadelphia': 581, 'away': 582, 'b': 583, 'vacation': 584, 'wouldnt': 585, 'funny': 586, 'couple': 587, 'dress': 588, 'yours': 589, 'nc': 590, 'farm': 591, 'working': 592, 'dog': 593, 'mine': 594, 'dream': 595, 'met': 596, 'ago': 597, 'sunny': 598, 'kansas': 599, 'trying': 600, 'station': 601, 'festival': 602, 'nofilter': 603, 'snow': 604, 'colorado': 605, 'o': 606, 'detroit': 607, 'winter': 608, 'union': 609, 'boo': 610, 'main': 611, 'football': 612, 'pictures': 613, 'own': 614, 'convention': 615, 'company': 616, 'win': 617, 'wish': 618, 'liberty': 619, 'universal': 620, 'help': 621, 'ball': 622, 'both': 623, 'feels': 624, 'spent': 625, 'vibes': 626, 'lmao': 627, 'style': 628, 'job': 629, 'l': 630, 'sorry': 631, 'reunited': 632, 'todays': 633, 'grateful': 634, 'drive': 635, 'wild': 636, 'handsome': 637, 'care': 638, 'sky': 639, 'future': 640, 'community': 641, 'ride': 642, 'd': 643, 'enough': 644, 'minnesota': 645, 'eyes': 646, 'weeks': 647, 'support': 648, 'cousins': 649, 'prom': 650, 'model': 651, 'anniversary': 652, 'madison': 653, 'song': 654, 'living': 655, 'road': 656, 'golf': 657, 'happiness': 658, 'chicken': 659, 'inn': 660, 'soul': 661, 'office': 662, 'fair': 663, 'golden': 664, 'ft': 665, 'pizza': 666, 'omg': 667, 'cleveland': 668, 'eve': 669, 'denver': 670, 'sports': 671, 'casino': 672, 'fuck': 673, 'canyon': 674, 'adventures': 675, 'cousin': 676, 'louisiana': 677, 'behind': 678, 'wine': 679, 'gift': 680, 'john': 681, 'iowa': 682, 'kitchen': 683, 'saint': 684, 'hanging': 685, 'columbia': 686, 'cold': 687, 'try': 688, 'half': 689, 'anyone': 690, 'crew': 691, 'color': 692, 'makeup': 693, 'sunshine': 694, 'snapchat': 695, 'yet': 696, 'line': 697, 'young': 698, 'wanted': 699, 'sushi': 700, 'hour': 701, 'losangeles': 702, 'mama': 703, 'dreams': 704, 'm': 705, 'tour': 706, 'lost': 707, 'star': 708, 'water': 709, 'spot': 710, 'bestie': 711, 'open': 712, 'air': 713, 'left': 714, 'each': 715, 'beer': 716, 'f': 717, 'happiest': 718, 'saw': 719, 'double': 720, 'epcot': 721, 'photos': 722, 'r': 723, 'seen': 724, 'afternoon': 725, 'fine': 726, 'latergram': 727, 'everyday': 728, 'goes': 729, 'spending': 730, 'camp': 731, 'co': 732, 'portland': 733, 'pics': 734, 'cream': 735, 'actually': 736, 'book': 737, 'stage': 738, 'car': 739, 'yoga': 740, 'harbor': 741, 'avenue': 742, 'memories': 743, 'pool': 744, 'grove': 745, 'ask': 746, 'kid': 747, 'charlotte': 748, 'historic': 749, 'damn': 750, 'king': 751, 'second': 752, 'needed': 753, 'mean': 754, 'de': 755, 'hours': 756, 'pub': 757, 'sis': 758, 'fresh': 759, 'cut': 760, 'drink': 761, 'bring': 762, 'tea': 763, 'absolutely': 764, 'thursday': 765, 'em': 766, 'space': 767, 'catch': 768, 'whats': 769, 'spa': 770, 'babies': 771, 'later': 772, 'gym': 773, 'course': 774, 'fathers': 775, 'tuesday': 776, 'quality': 777, 'states': 778, 'comes': 779, 'salon': 780, 'share': 781, 'oregon': 782, 'front': 783, 'close': 784, 'missouri': 785, 'plaza': 786, 'nature': 787, 'meeting': 788, 'factory': 789, 'earth': 790, 'mind': 791, 'bitch': 792, 'views': 793, 'mi': 794, 'four': 795, 'ones': 796, 'area': 797, 'forget': 798, 'ok': 799, 'warm': 800, 'gets': 801, 'evening': 802, 'business': 803, 'academy': 804, 'sea': 805, 'fan': 806, 'bestfriend': 807, 'c': 808, 'empire': 809, 'kind': 810, 'son': 811, 'says': 812, 'babes': 813, 'theater': 814, 'delta': 815, 'el': 816, 'natural': 817, 'needs': 818, 'bit': 819, 'month': 820, 'name': 821, 'hilton': 822, 'queen': 823, 'mothers': 824, 'movie': 825, 'leave': 826, 'yourself': 827, 'charleston': 828, 'literally': 829, 'vs': 830, 'far': 831, 'eat': 832, 'told': 833, 'dope': 834, 'alpha': 835, 'phoenix': 836, 'da': 837, 'joy': 838, 'experience': 839, 'mood': 840, 'daddy': 841, 'bc': 842, 'mr': 843, 'rain': 844, 'ma': 845, 'columbus': 846, 'nj': 847, 'woman': 848, 'straight': 849, 'smiles': 850, 'campus': 851, 'antonio': 852, 'workout': 853, 'bowl': 854, 'definitely': 855, 'might': 856, 'venice': 857, 'hate': 858, 'philly': 859, 'words': 860, 'hospital': 861, 'started': 862, 'wisconsin': 863, 'pink': 864, 'tb': 865, 'pare': 866, 'lincoln': 867, 'swear': 868, 'till': 869, 'middle': 870, 'momma': 871, 'anything': 872, 'myrtle': 873, 'magical': 874, 'orange': 875, 'beat': 876, 'wit': 877, 'change': 878, 'mother': 879, 'bout': 880, 'talk': 881, 'during': 882, 'congratulations': 883, 'gardens': 884, 'turn': 885, 'nd': 886, 'brought': 887, 'connecticut': 888, 'mount': 889, 'phone': 890, 'outside': 891, 'princess': 892, 'yeah': 893, 'six': 894, 'forest': 895, 'obsessed': 896, 'beverly': 897, 'heights': 898, 'apartments': 899, 'hand': 900, 'baltimore': 901, 'past': 902, 'newport': 903, 'surprise': 904, 'oklahoma': 905, 'running': 906, 'money': 907, 'gave': 908, 'wings': 909, 'rest': 910, 'walking': 911, 'nots': 912, 'delicious': 913, 'ranch': 914, 'monica': 915, 'wear': 916, 'buffalo': 917, 'reason': 918, 'wednesday': 919, 'tattoo': 920, 'phi': 921, 'tower': 922, 'yo': 923, 'cake': 924, 'cutie': 925, 'tavern': 926, 'canada': 927, 'wearing': 928, 'g': 929, 'nigga': 930, 'row': 931, 'brothers': 932, 'sanfrancisco': 933, 'photoshoot': 934, 'honor': 935, 'daughter': 936, 'fucking': 937, 'story': 938, 'shoutout': 939, 'starting': 940, 'sleep': 941, 'chocolate': 942, 'cheers': 943, 'y': 944, 'whos': 945, 'centre': 946, 'palm': 947, 'seriously': 948, 'goals': 949, 'boyfriend': 950, 'library': 951, 'ur': 952, 'wont': 953, 'homecoming': 954, 'group': 955, 'band': 956, 'join': 957, 'candy': 958, 'biggest': 959, 'giving': 960, 'laugh': 961, 'southern': 962, 'buddy': 963, 'husband': 964, 'heaven': 965, 'rt': 966, 'bear': 967, 'pa': 968, 'cheesecake': 969, 'ga': 970, 'sign': 971, 'theyre': 972}\n"
     ]
    }
   ],
   "source": [
    "print(train_voc.token_to_idx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "global train_voc\n",
    "test_raw_data = pd.read_csv(TEST_FILE)\n",
    "train_text_list = list()\n",
    "\n",
    "for single_text in test_raw_data.Tweet:\n",
    "    single_text = re.sub(r'[\\W]+', ' ', single_text.lower())\n",
    "    single_text = single_text.replace('hadn t', 'had not')\n",
    "    single_text = single_text.replace('wasn t', 'was not')\n",
    "    single_text = single_text.replace('didn t', 'did not')\n",
    "    single_text = single_text.replace('havent', 'have not')\n",
    "    single_text = single_text.replace('isn t', 'is not')\n",
    "    single_text = single_text.replace('isnt', 'is not')\n",
    "    single_text = single_text.replace('doesnt', 'does not')\n",
    "    single_text = single_text.replace('dont', 'do not')\n",
    "    single_text = single_text.replace('hasnt', 'has not')\n",
    "    single_text = single_text.replace('arent', 'are not')\n",
    "    single_text = single_text.replace('•', '')\n",
    "    single_text = single_text.replace('••', '')\n",
    "    single_text = single_text.replace('  ', ' ')\n",
    "    single_text = single_text.replace('   ', ' ')\n",
    "\n",
    "    train_text_list.append(single_text)\n",
    "\n",
    "list_init = list()\n",
    "for i in range(len(test_raw_data)):\n",
    "    list_init.append(np.zeros(len(train_voc) - 1))\n",
    "\n",
    "test_cleaned = test_raw_data\n",
    "test_cleaned['tokens'] = list_init\n",
    "\n",
    "list_init.clear()\n",
    "for i in range(len(test_raw_data)):\n",
    "    list_init.append(np.zeros(20))\n",
    "\n",
    "test_cleaned['emo_vecs'] = list_init\n",
    "\n",
    "test_tokens = np.zeros((len(test_raw_data), len(train_voc) - 1))\n",
    "test_emos = np.zeros((len(test_raw_data), 20))\n",
    "\n",
    "for i in range(len(test_cleaned)):\n",
    "    cur_text = test_cleaned.iloc[i]['Tweet']\n",
    "    cur_words = cur_text.split()\n",
    "    cur_indices = np.zeros(len(train_voc) - 1)\n",
    "\n",
    "    for word in cur_words:\n",
    "        temp_index = train_voc[word]\n",
    "        if temp_index == 0:\n",
    "            continue\n",
    "        cur_indices[temp_index - 1] += 1\n",
    "\n",
    "    test_cleaned.at[i, 'tokens'] = cur_indices\n",
    "    test_tokens[i] = cur_indices\n",
    "\n",
    "    cur_emos = np.zeros(20)\n",
    "    cur_emos[test_cleaned.iloc[i]['Label']] = 1\n",
    "\n",
    "    test_cleaned.at[i, 'emo_vecs'] = cur_emos\n",
    "    test_emos[i] = cur_emos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(                                                  Tweet  Label  \\\n 0     pho mi amor pho oceanbeach tourunicos siddhart...      1   \n 1     this made possible by a beautiful lady thanks ...      1   \n 2     throwback to  when i went to detroit amp got k...      2   \n 3     obamas last tree lighting  national christmas ...     17   \n 4     kicking the new year off right w my bff  shipp...      0   \n ...                                                 ...    ...   \n 9994  my ootd love this chain so much and our new ri...      1   \n 9995  met santa and olaf  the north pole today  nort...      0   \n 9996  new york by night strideby herelocationnyc see...     11   \n 9997    kisses for the birthday girl  helzberg diamonds      0   \n 9998  dinner with this priceless viewthank you s ann...      9   \n \n                                                  tokens  \\\n 0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n 1     [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...   \n 2     [0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n 3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n 4     [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n ...                                                 ...   \n 9994  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...   \n 9995  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...   \n 9996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n 9997  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n 9998  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...   \n \n                                                emo_vecs  \n 0     [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n 1     [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n 2     [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n 3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n 4     [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n ...                                                 ...  \n 9994  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n 9995  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n 9996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n 9997  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n 9998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n \n [9999 rows x 4 columns],\n array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 2., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [1., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]),\n array([[0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 0., 1., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [1., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cleaned, test_tokens, test_emos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def get_accuracy(Y_pred, Y):\n",
    "    ct, ttl_ct = 0, 0\n",
    "    for i in range(len(Y_pred)):\n",
    "        ct += (np.argmax(Y_pred[i]) == np.argmax(Y[i]))\n",
    "        ttl_ct += 1\n",
    "    return ct / ttl_ct"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "num_data = len(clean_tokens)\n",
    "x_size = len(clean_tokens[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_in, n_h1, n_h2, n_h3, n_out = x_size, 600, 300, 100, 20"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "lr = 0.04"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(n_in, n_h1),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_h1, n_h2),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_h2, n_h3),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_h3, n_out),\n",
    "                      nn.Sigmoid())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "x = torch.from_numpy(clean_tokens)\n",
    "x = x.float()\n",
    "y = torch.tensor(clean_emos)\n",
    "y = y.float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"acc\")\n",
    "epoch_list = []\n",
    "acc_list = []\n",
    "acc_list_test = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.25629091262817383  accuracy:  0.0336\n",
      "epoch:  1  loss:  0.2561592757701874  accuracy:  0.033725\n",
      "epoch:  2  loss:  0.2560276687145233  accuracy:  0.033925\n",
      "epoch:  3  loss:  0.25589612126350403  accuracy:  0.03405\n",
      "epoch:  4  loss:  0.2557646334171295  accuracy:  0.0339\n",
      "epoch:  5  loss:  0.255633145570755  accuracy:  0.03395\n",
      "epoch:  6  loss:  0.25550171732902527  accuracy:  0.033925\n",
      "epoch:  7  loss:  0.2553703188896179  accuracy:  0.033775\n",
      "epoch:  8  loss:  0.25523898005485535  accuracy:  0.0338\n",
      "epoch:  9  loss:  0.25510767102241516  accuracy:  0.033875\n",
      "epoch:  10  loss:  0.254976361989975  accuracy:  0.033925\n",
      "epoch:  11  loss:  0.25484511256217957  accuracy:  0.033825\n",
      "epoch:  12  loss:  0.25471386313438416  accuracy:  0.0339\n",
      "epoch:  13  loss:  0.25458261370658875  accuracy:  0.033975\n",
      "epoch:  14  loss:  0.2544513940811157  accuracy:  0.0341\n",
      "epoch:  15  loss:  0.2543202042579651  accuracy:  0.0341\n",
      "epoch:  16  loss:  0.25418901443481445  accuracy:  0.034175\n",
      "epoch:  17  loss:  0.2540578842163086  accuracy:  0.03425\n",
      "epoch:  18  loss:  0.25392675399780273  accuracy:  0.03425\n",
      "epoch:  19  loss:  0.2537956237792969  accuracy:  0.034275\n",
      "epoch:  20  loss:  0.253664493560791  accuracy:  0.0344\n",
      "epoch:  21  loss:  0.25353342294692993  accuracy:  0.034475\n",
      "epoch:  22  loss:  0.25340235233306885  accuracy:  0.0346\n",
      "epoch:  23  loss:  0.25327128171920776  accuracy:  0.03465\n",
      "epoch:  24  loss:  0.25314027070999146  accuracy:  0.0348\n",
      "epoch:  25  loss:  0.25300928950309753  accuracy:  0.03495\n",
      "epoch:  26  loss:  0.2528783082962036  accuracy:  0.0349\n",
      "epoch:  27  loss:  0.25274738669395447  accuracy:  0.0349\n",
      "epoch:  28  loss:  0.2526164948940277  accuracy:  0.035025\n",
      "epoch:  29  loss:  0.25248557329177856  accuracy:  0.0349\n",
      "epoch:  30  loss:  0.2523547410964966  accuracy:  0.035025\n",
      "epoch:  31  loss:  0.252223938703537  accuracy:  0.03505\n",
      "epoch:  32  loss:  0.25209319591522217  accuracy:  0.03505\n",
      "epoch:  33  loss:  0.25196245312690735  accuracy:  0.035\n",
      "epoch:  34  loss:  0.2518318295478821  accuracy:  0.035\n",
      "epoch:  35  loss:  0.2517012059688568  accuracy:  0.0351\n",
      "epoch:  36  loss:  0.25157061219215393  accuracy:  0.035075\n",
      "epoch:  37  loss:  0.2514401078224182  accuracy:  0.03515\n",
      "epoch:  38  loss:  0.25130966305732727  accuracy:  0.035225\n",
      "epoch:  39  loss:  0.2511792480945587  accuracy:  0.035275\n",
      "epoch:  40  loss:  0.25104889273643494  accuracy:  0.035425\n",
      "epoch:  41  loss:  0.25091859698295593  accuracy:  0.035525\n",
      "epoch:  42  loss:  0.2507883906364441  accuracy:  0.03565\n",
      "epoch:  43  loss:  0.25065821409225464  accuracy:  0.035675\n",
      "epoch:  44  loss:  0.2505280673503876  accuracy:  0.035675\n",
      "epoch:  45  loss:  0.25039801001548767  accuracy:  0.03565\n",
      "epoch:  46  loss:  0.25026801228523254  accuracy:  0.03565\n",
      "epoch:  47  loss:  0.2501380443572998  accuracy:  0.0357\n",
      "epoch:  48  loss:  0.25000816583633423  accuracy:  0.0357\n",
      "epoch:  49  loss:  0.24987836182117462  accuracy:  0.0358\n",
      "epoch:  50  loss:  0.2497485876083374  accuracy:  0.035825\n",
      "epoch:  51  loss:  0.24961887300014496  accuracy:  0.0359\n",
      "epoch:  52  loss:  0.24948926270008087  accuracy:  0.035925\n",
      "epoch:  53  loss:  0.24935965240001678  accuracy:  0.036\n",
      "epoch:  54  loss:  0.24923013150691986  accuracy:  0.036\n",
      "epoch:  55  loss:  0.24910064041614532  accuracy:  0.03605\n",
      "epoch:  56  loss:  0.24897125363349915  accuracy:  0.036075\n",
      "epoch:  57  loss:  0.24884188175201416  accuracy:  0.036075\n",
      "epoch:  58  loss:  0.24871259927749634  accuracy:  0.0361\n",
      "epoch:  59  loss:  0.2485833615064621  accuracy:  0.03615\n",
      "epoch:  60  loss:  0.24845418334007263  accuracy:  0.03615\n",
      "epoch:  61  loss:  0.24832507967948914  accuracy:  0.036075\n",
      "epoch:  62  loss:  0.24819597601890564  accuracy:  0.036075\n",
      "epoch:  63  loss:  0.2480669915676117  accuracy:  0.0361\n",
      "epoch:  64  loss:  0.24793800711631775  accuracy:  0.036125\n",
      "epoch:  65  loss:  0.24780912697315216  accuracy:  0.036125\n",
      "epoch:  66  loss:  0.24768027663230896  accuracy:  0.0361\n",
      "epoch:  67  loss:  0.24755150079727173  accuracy:  0.036125\n",
      "epoch:  68  loss:  0.24742276966571808  accuracy:  0.0361\n",
      "epoch:  69  loss:  0.24729406833648682  accuracy:  0.036125\n",
      "epoch:  70  loss:  0.24716542661190033  accuracy:  0.0361\n",
      "epoch:  71  loss:  0.2470368593931198  accuracy:  0.0361\n",
      "epoch:  72  loss:  0.24690832197666168  accuracy:  0.0361\n",
      "epoch:  73  loss:  0.24677982926368713  accuracy:  0.0361\n",
      "epoch:  74  loss:  0.24665138125419617  accuracy:  0.036075\n",
      "epoch:  75  loss:  0.24652299284934998  accuracy:  0.036075\n",
      "epoch:  76  loss:  0.24639466404914856  accuracy:  0.0361\n",
      "epoch:  77  loss:  0.24626637995243073  accuracy:  0.0361\n",
      "epoch:  78  loss:  0.24613812565803528  accuracy:  0.036075\n",
      "epoch:  79  loss:  0.2460099458694458  accuracy:  0.03605\n",
      "epoch:  80  loss:  0.2458818107843399  accuracy:  0.036075\n",
      "epoch:  81  loss:  0.2457537055015564  accuracy:  0.036075\n",
      "epoch:  82  loss:  0.24562565982341766  accuracy:  0.0361\n",
      "epoch:  83  loss:  0.24549764394760132  accuracy:  0.0361\n",
      "epoch:  84  loss:  0.24536968767642975  accuracy:  0.0361\n",
      "epoch:  85  loss:  0.24524177610874176  accuracy:  0.03615\n",
      "epoch:  86  loss:  0.24511392414569855  accuracy:  0.036125\n",
      "epoch:  87  loss:  0.24498608708381653  accuracy:  0.03615\n",
      "epoch:  88  loss:  0.24485833942890167  accuracy:  0.036125\n",
      "epoch:  89  loss:  0.24473059177398682  accuracy:  0.0361\n",
      "epoch:  90  loss:  0.24460288882255554  accuracy:  0.0361\n",
      "epoch:  91  loss:  0.24447526037693024  accuracy:  0.036075\n",
      "epoch:  92  loss:  0.24434763193130493  accuracy:  0.0361\n",
      "epoch:  93  loss:  0.24422012269496918  accuracy:  0.0361\n",
      "epoch:  94  loss:  0.24409258365631104  accuracy:  0.03605\n",
      "epoch:  95  loss:  0.24396511912345886  accuracy:  0.03605\n",
      "epoch:  96  loss:  0.24383766949176788  accuracy:  0.0361\n",
      "epoch:  97  loss:  0.24371029436588287  accuracy:  0.0361\n",
      "epoch:  98  loss:  0.24358294904232025  accuracy:  0.036075\n",
      "epoch:  99  loss:  0.2434556484222412  accuracy:  0.0361\n",
      "epoch:  100  loss:  0.24332837760448456  accuracy:  0.0361\n",
      "epoch:  101  loss:  0.24320119619369507  accuracy:  0.0361\n",
      "epoch:  102  loss:  0.2430739849805832  accuracy:  0.0361\n",
      "epoch:  103  loss:  0.24294687807559967  accuracy:  0.036125\n",
      "epoch:  104  loss:  0.24281978607177734  accuracy:  0.036125\n",
      "epoch:  105  loss:  0.2426927387714386  accuracy:  0.0361\n",
      "epoch:  106  loss:  0.24256573617458344  accuracy:  0.036125\n",
      "epoch:  107  loss:  0.24243874847888947  accuracy:  0.03615\n",
      "epoch:  108  loss:  0.24231185019016266  accuracy:  0.036175\n",
      "epoch:  109  loss:  0.24218496680259705  accuracy:  0.036175\n",
      "epoch:  110  loss:  0.24205809831619263  accuracy:  0.036175\n",
      "epoch:  111  loss:  0.24193130433559418  accuracy:  0.036175\n",
      "epoch:  112  loss:  0.2418045550584793  accuracy:  0.036175\n",
      "epoch:  113  loss:  0.24167780578136444  accuracy:  0.03615\n",
      "epoch:  114  loss:  0.24155113101005554  accuracy:  0.036175\n",
      "epoch:  115  loss:  0.24142451584339142  accuracy:  0.0362\n",
      "epoch:  116  loss:  0.2412978857755661  accuracy:  0.0362\n",
      "epoch:  117  loss:  0.24117133021354675  accuracy:  0.036175\n",
      "epoch:  118  loss:  0.2410448044538498  accuracy:  0.0362\n",
      "epoch:  119  loss:  0.2409183382987976  accuracy:  0.0362\n",
      "epoch:  120  loss:  0.2407919019460678  accuracy:  0.0362\n",
      "epoch:  121  loss:  0.2406655102968216  accuracy:  0.0362\n",
      "epoch:  122  loss:  0.24053917825222015  accuracy:  0.0362\n",
      "epoch:  123  loss:  0.2404128462076187  accuracy:  0.036175\n",
      "epoch:  124  loss:  0.24028660356998444  accuracy:  0.036175\n",
      "epoch:  125  loss:  0.24016037583351135  accuracy:  0.036175\n",
      "epoch:  126  loss:  0.24003417789936066  accuracy:  0.036175\n",
      "epoch:  127  loss:  0.23990803956985474  accuracy:  0.036175\n",
      "epoch:  128  loss:  0.2397819459438324  accuracy:  0.036175\n",
      "epoch:  129  loss:  0.23965588212013245  accuracy:  0.036175\n",
      "epoch:  130  loss:  0.23952984809875488  accuracy:  0.0362\n",
      "epoch:  131  loss:  0.2394038438796997  accuracy:  0.03625\n",
      "epoch:  132  loss:  0.2392779141664505  accuracy:  0.036275\n",
      "epoch:  133  loss:  0.2391519993543625  accuracy:  0.036275\n",
      "epoch:  134  loss:  0.23902611434459686  accuracy:  0.036325\n",
      "epoch:  135  loss:  0.238900288939476  accuracy:  0.036325\n",
      "epoch:  136  loss:  0.23877450823783875  accuracy:  0.036375\n",
      "epoch:  137  loss:  0.23864875733852386  accuracy:  0.036375\n",
      "epoch:  138  loss:  0.23852300643920898  accuracy:  0.036425\n",
      "epoch:  139  loss:  0.23839733004570007  accuracy:  0.036425\n",
      "epoch:  140  loss:  0.23827168345451355  accuracy:  0.036525\n",
      "epoch:  141  loss:  0.238146111369133  accuracy:  0.03655\n",
      "epoch:  142  loss:  0.23802052438259125  accuracy:  0.036625\n",
      "epoch:  143  loss:  0.23789499700069427  accuracy:  0.03665\n",
      "epoch:  144  loss:  0.23776951432228088  accuracy:  0.036775\n",
      "epoch:  145  loss:  0.2376440167427063  accuracy:  0.036825\n",
      "epoch:  146  loss:  0.23751863837242126  accuracy:  0.0369\n",
      "epoch:  147  loss:  0.23739321529865265  accuracy:  0.036975\n",
      "epoch:  148  loss:  0.2372678518295288  accuracy:  0.037125\n",
      "epoch:  149  loss:  0.23714253306388855  accuracy:  0.03725\n",
      "epoch:  150  loss:  0.23701724410057068  accuracy:  0.0374\n",
      "epoch:  151  loss:  0.2368919849395752  accuracy:  0.037725\n",
      "epoch:  152  loss:  0.2367667555809021  accuracy:  0.038075\n",
      "epoch:  153  loss:  0.2366415560245514  accuracy:  0.0383\n",
      "epoch:  154  loss:  0.23651640117168427  accuracy:  0.038525\n",
      "epoch:  155  loss:  0.23639124631881714  accuracy:  0.038725\n",
      "epoch:  156  loss:  0.23626616597175598  accuracy:  0.038975\n",
      "epoch:  157  loss:  0.23614107072353363  accuracy:  0.0395\n",
      "epoch:  158  loss:  0.23601603507995605  accuracy:  0.039925\n",
      "epoch:  159  loss:  0.23589101433753967  accuracy:  0.0404\n",
      "epoch:  160  loss:  0.23576603829860687  accuracy:  0.041125\n",
      "epoch:  161  loss:  0.23564104735851288  accuracy:  0.04165\n",
      "epoch:  162  loss:  0.23551613092422485  accuracy:  0.04235\n",
      "epoch:  163  loss:  0.23539121448993683  accuracy:  0.042975\n",
      "epoch:  164  loss:  0.23526631295681  accuracy:  0.04385\n",
      "epoch:  165  loss:  0.23514142632484436  accuracy:  0.04475\n",
      "epoch:  166  loss:  0.2350165843963623  accuracy:  0.04565\n",
      "epoch:  167  loss:  0.23489175736904144  accuracy:  0.0465\n",
      "epoch:  168  loss:  0.23476696014404297  accuracy:  0.0477\n",
      "epoch:  169  loss:  0.2346421629190445  accuracy:  0.048825\n",
      "epoch:  170  loss:  0.23451738059520721  accuracy:  0.05015\n",
      "epoch:  171  loss:  0.23439261317253113  accuracy:  0.05185\n",
      "epoch:  172  loss:  0.23426789045333862  accuracy:  0.0537\n",
      "epoch:  173  loss:  0.2341431826353073  accuracy:  0.055475\n",
      "epoch:  174  loss:  0.234018474817276  accuracy:  0.0574\n",
      "epoch:  175  loss:  0.23389378190040588  accuracy:  0.059375\n",
      "epoch:  176  loss:  0.23376911878585815  accuracy:  0.062425\n",
      "epoch:  177  loss:  0.2336444854736328  accuracy:  0.064525\n",
      "epoch:  178  loss:  0.23351983726024628  accuracy:  0.0667\n",
      "epoch:  179  loss:  0.23339521884918213  accuracy:  0.0691\n",
      "epoch:  180  loss:  0.23327063024044037  accuracy:  0.071925\n",
      "epoch:  181  loss:  0.23314599692821503  accuracy:  0.0749\n",
      "epoch:  182  loss:  0.23302142322063446  accuracy:  0.07855\n",
      "epoch:  183  loss:  0.2328968346118927  accuracy:  0.08195\n",
      "epoch:  184  loss:  0.23277224600315094  accuracy:  0.08515\n",
      "epoch:  185  loss:  0.23264773190021515  accuracy:  0.088275\n",
      "epoch:  186  loss:  0.2325231432914734  accuracy:  0.0921\n",
      "epoch:  187  loss:  0.2323986291885376  accuracy:  0.095775\n",
      "epoch:  188  loss:  0.23227405548095703  accuracy:  0.099875\n",
      "epoch:  189  loss:  0.23214955627918243  accuracy:  0.10405\n",
      "epoch:  190  loss:  0.23202501237392426  accuracy:  0.108625\n",
      "epoch:  191  loss:  0.23190048336982727  accuracy:  0.113175\n",
      "epoch:  192  loss:  0.23177596926689148  accuracy:  0.117625\n",
      "epoch:  193  loss:  0.23165148496627808  accuracy:  0.12125\n",
      "epoch:  194  loss:  0.2315269559621811  accuracy:  0.128075\n",
      "epoch:  195  loss:  0.2314024567604065  accuracy:  0.13215\n",
      "epoch:  196  loss:  0.2312779426574707  accuracy:  0.1364\n",
      "epoch:  197  loss:  0.2311534583568573  accuracy:  0.14005\n",
      "epoch:  198  loss:  0.2310289591550827  accuracy:  0.144625\n",
      "epoch:  199  loss:  0.2309044599533081  accuracy:  0.149225\n",
      "epoch:  200  loss:  0.2307799756526947  accuracy:  0.15315\n",
      "epoch:  201  loss:  0.2306554615497589  accuracy:  0.1572\n",
      "epoch:  202  loss:  0.2305309921503067  accuracy:  0.1614\n",
      "epoch:  203  loss:  0.2304064780473709  accuracy:  0.1651\n",
      "epoch:  204  loss:  0.2302819788455963  accuracy:  0.16885\n",
      "epoch:  205  loss:  0.23015747964382172  accuracy:  0.17165\n",
      "epoch:  206  loss:  0.23003296554088593  accuracy:  0.174625\n",
      "epoch:  207  loss:  0.22990848124027252  accuracy:  0.1775\n",
      "epoch:  208  loss:  0.22978395223617554  accuracy:  0.18\n",
      "epoch:  209  loss:  0.22965945303440094  accuracy:  0.18205\n",
      "epoch:  210  loss:  0.22953492403030396  accuracy:  0.184325\n",
      "epoch:  211  loss:  0.22941039502620697  accuracy:  0.186525\n",
      "epoch:  212  loss:  0.22928586602210999  accuracy:  0.1885\n",
      "epoch:  213  loss:  0.2291613221168518  accuracy:  0.190075\n",
      "epoch:  214  loss:  0.22903676331043243  accuracy:  0.191825\n",
      "epoch:  215  loss:  0.22891220450401306  accuracy:  0.193675\n",
      "epoch:  216  loss:  0.2287876307964325  accuracy:  0.195225\n",
      "epoch:  217  loss:  0.22866308689117432  accuracy:  0.196325\n",
      "epoch:  218  loss:  0.22853848338127136  accuracy:  0.197975\n",
      "epoch:  219  loss:  0.2284139096736908  accuracy:  0.1993\n",
      "epoch:  220  loss:  0.22828929126262665  accuracy:  0.200425\n",
      "epoch:  221  loss:  0.2281646877527237  accuracy:  0.201725\n",
      "epoch:  222  loss:  0.22804008424282074  accuracy:  0.2027\n",
      "epoch:  223  loss:  0.2279154509305954  accuracy:  0.203725\n",
      "epoch:  224  loss:  0.22779078781604767  accuracy:  0.2047\n",
      "epoch:  225  loss:  0.22766610980033875  accuracy:  0.2055\n",
      "epoch:  226  loss:  0.2275414615869522  accuracy:  0.206425\n",
      "epoch:  227  loss:  0.2274167835712433  accuracy:  0.2071\n",
      "epoch:  228  loss:  0.22729209065437317  accuracy:  0.208\n",
      "epoch:  229  loss:  0.22716738283634186  accuracy:  0.20875\n",
      "epoch:  230  loss:  0.22704266011714935  accuracy:  0.209375\n",
      "epoch:  231  loss:  0.22691792249679565  accuracy:  0.21005\n",
      "epoch:  232  loss:  0.22679318487644196  accuracy:  0.21055\n",
      "epoch:  233  loss:  0.22666841745376587  accuracy:  0.211\n",
      "epoch:  234  loss:  0.22654365003108978  accuracy:  0.21135\n",
      "epoch:  235  loss:  0.22641882300376892  accuracy:  0.211625\n",
      "epoch:  236  loss:  0.22629402577877045  accuracy:  0.211925\n",
      "epoch:  237  loss:  0.22616919875144958  accuracy:  0.21215\n",
      "epoch:  238  loss:  0.22604437172412872  accuracy:  0.212625\n",
      "epoch:  239  loss:  0.22591951489448547  accuracy:  0.212975\n",
      "epoch:  240  loss:  0.22579462826251984  accuracy:  0.21325\n",
      "epoch:  241  loss:  0.225669726729393  accuracy:  0.2135\n",
      "epoch:  242  loss:  0.22554482519626617  accuracy:  0.21375\n",
      "epoch:  243  loss:  0.22541987895965576  accuracy:  0.214125\n",
      "epoch:  244  loss:  0.22529494762420654  accuracy:  0.2143\n",
      "epoch:  245  loss:  0.22516995668411255  accuracy:  0.2144\n",
      "epoch:  246  loss:  0.22504498064517975  accuracy:  0.214575\n",
      "epoch:  247  loss:  0.22491995990276337  accuracy:  0.214825\n",
      "epoch:  248  loss:  0.22479493916034698  accuracy:  0.214925\n",
      "epoch:  249  loss:  0.22466985881328583  accuracy:  0.215075\n",
      "epoch:  250  loss:  0.22454482316970825  accuracy:  0.21515\n",
      "epoch:  251  loss:  0.2244197279214859  accuracy:  0.21535\n",
      "epoch:  252  loss:  0.22429458796977997  accuracy:  0.21555\n",
      "epoch:  253  loss:  0.22416947782039642  accuracy:  0.215675\n",
      "epoch:  254  loss:  0.2240443229675293  accuracy:  0.21575\n",
      "epoch:  255  loss:  0.2239191085100174  accuracy:  0.21585\n",
      "epoch:  256  loss:  0.2237939089536667  accuracy:  0.2159\n",
      "epoch:  257  loss:  0.2236686646938324  accuracy:  0.215975\n",
      "epoch:  258  loss:  0.2235434204339981  accuracy:  0.21595\n",
      "epoch:  259  loss:  0.22341813147068024  accuracy:  0.215975\n",
      "epoch:  260  loss:  0.22329282760620117  accuracy:  0.216\n",
      "epoch:  261  loss:  0.22316749393939972  accuracy:  0.21605\n",
      "epoch:  262  loss:  0.22304213047027588  accuracy:  0.216075\n",
      "epoch:  263  loss:  0.22291675209999084  accuracy:  0.216075\n",
      "epoch:  264  loss:  0.22279132902622223  accuracy:  0.216125\n",
      "epoch:  265  loss:  0.22266587615013123  accuracy:  0.2162\n",
      "epoch:  266  loss:  0.22254040837287903  accuracy:  0.216275\n",
      "epoch:  267  loss:  0.22241488099098206  accuracy:  0.2163\n",
      "epoch:  268  loss:  0.2222893387079239  accuracy:  0.21635\n",
      "epoch:  269  loss:  0.22216375172138214  accuracy:  0.2164\n",
      "epoch:  270  loss:  0.2220381647348404  accuracy:  0.2164\n",
      "epoch:  271  loss:  0.22191251814365387  accuracy:  0.2164\n",
      "epoch:  272  loss:  0.22178684175014496  accuracy:  0.2164\n",
      "epoch:  273  loss:  0.22166113555431366  accuracy:  0.2164\n",
      "epoch:  274  loss:  0.22153538465499878  accuracy:  0.216425\n",
      "epoch:  275  loss:  0.2214096039533615  accuracy:  0.216475\n",
      "epoch:  276  loss:  0.22128380835056305  accuracy:  0.216525\n",
      "epoch:  277  loss:  0.22115792334079742  accuracy:  0.216525\n",
      "epoch:  278  loss:  0.2210320681333542  accuracy:  0.216575\n",
      "epoch:  279  loss:  0.2209061086177826  accuracy:  0.216575\n",
      "epoch:  280  loss:  0.220780149102211  accuracy:  0.216575\n",
      "epoch:  281  loss:  0.22065414488315582  accuracy:  0.216575\n",
      "epoch:  282  loss:  0.22052812576293945  accuracy:  0.216575\n",
      "epoch:  283  loss:  0.22040203213691711  accuracy:  0.216575\n",
      "epoch:  284  loss:  0.22027587890625  accuracy:  0.216575\n",
      "epoch:  285  loss:  0.2201497107744217  accuracy:  0.2166\n",
      "epoch:  286  loss:  0.2200234979391098  accuracy:  0.216625\n",
      "epoch:  287  loss:  0.21989722549915314  accuracy:  0.216625\n",
      "epoch:  288  loss:  0.21977092325687408  accuracy:  0.216625\n",
      "epoch:  289  loss:  0.21964457631111145  accuracy:  0.216625\n",
      "epoch:  290  loss:  0.21951816976070404  accuracy:  0.216625\n",
      "epoch:  291  loss:  0.21939171850681305  accuracy:  0.216625\n",
      "epoch:  292  loss:  0.21926525235176086  accuracy:  0.216625\n",
      "epoch:  293  loss:  0.21913869678974152  accuracy:  0.216625\n",
      "epoch:  294  loss:  0.2190120965242386  accuracy:  0.216625\n",
      "epoch:  295  loss:  0.21888545155525208  accuracy:  0.216625\n",
      "epoch:  296  loss:  0.21875879168510437  accuracy:  0.21665\n",
      "epoch:  297  loss:  0.2186320275068283  accuracy:  0.21665\n",
      "epoch:  298  loss:  0.21850523352622986  accuracy:  0.21665\n",
      "epoch:  299  loss:  0.21837837994098663  accuracy:  0.21665\n",
      "epoch:  300  loss:  0.21825148165225983  accuracy:  0.21665\n",
      "epoch:  301  loss:  0.21812455356121063  accuracy:  0.21665\n",
      "epoch:  302  loss:  0.21799756586551666  accuracy:  0.21665\n",
      "epoch:  303  loss:  0.21787048876285553  accuracy:  0.21665\n",
      "epoch:  304  loss:  0.217743381857872  accuracy:  0.21665\n",
      "epoch:  305  loss:  0.2176162153482437  accuracy:  0.21665\n",
      "epoch:  306  loss:  0.21748895943164825  accuracy:  0.21665\n",
      "epoch:  307  loss:  0.2173617035150528  accuracy:  0.216675\n",
      "epoch:  308  loss:  0.21723434329032898  accuracy:  0.216675\n",
      "epoch:  309  loss:  0.21710693836212158  accuracy:  0.216675\n",
      "epoch:  310  loss:  0.2169794887304306  accuracy:  0.216675\n",
      "epoch:  311  loss:  0.21685197949409485  accuracy:  0.216675\n",
      "epoch:  312  loss:  0.21672439575195312  accuracy:  0.216675\n",
      "epoch:  313  loss:  0.21659673750400543  accuracy:  0.216675\n",
      "epoch:  314  loss:  0.21646901965141296  accuracy:  0.216675\n",
      "epoch:  315  loss:  0.2163412868976593  accuracy:  0.216675\n",
      "epoch:  316  loss:  0.2162134349346161  accuracy:  0.216675\n",
      "epoch:  317  loss:  0.2160855233669281  accuracy:  0.216675\n",
      "epoch:  318  loss:  0.21595758199691772  accuracy:  0.216675\n",
      "epoch:  319  loss:  0.215829536318779  accuracy:  0.216675\n",
      "epoch:  320  loss:  0.21570146083831787  accuracy:  0.216675\n",
      "epoch:  321  loss:  0.2155732959508896  accuracy:  0.216675\n",
      "epoch:  322  loss:  0.21544504165649414  accuracy:  0.216675\n",
      "epoch:  323  loss:  0.2153167575597763  accuracy:  0.216675\n",
      "epoch:  324  loss:  0.2151883989572525  accuracy:  0.216675\n",
      "epoch:  325  loss:  0.21505996584892273  accuracy:  0.216675\n",
      "epoch:  326  loss:  0.2149314433336258  accuracy:  0.216675\n",
      "epoch:  327  loss:  0.2148028463125229  accuracy:  0.216675\n",
      "epoch:  328  loss:  0.2146742045879364  accuracy:  0.216675\n",
      "epoch:  329  loss:  0.21454550325870514  accuracy:  0.216675\n",
      "epoch:  330  loss:  0.21441668272018433  accuracy:  0.216675\n",
      "epoch:  331  loss:  0.21428781747817993  accuracy:  0.216675\n",
      "epoch:  332  loss:  0.21415889263153076  accuracy:  0.216675\n",
      "epoch:  333  loss:  0.21402984857559204  accuracy:  0.216675\n",
      "epoch:  334  loss:  0.21390073001384735  accuracy:  0.216675\n",
      "epoch:  335  loss:  0.21377158164978027  accuracy:  0.216675\n",
      "epoch:  336  loss:  0.21364232897758484  accuracy:  0.216675\n",
      "epoch:  337  loss:  0.21351298689842224  accuracy:  0.216675\n",
      "epoch:  338  loss:  0.21338357031345367  accuracy:  0.216675\n",
      "epoch:  339  loss:  0.21325406432151794  accuracy:  0.216675\n",
      "epoch:  340  loss:  0.21312449872493744  accuracy:  0.216675\n",
      "epoch:  341  loss:  0.21299482882022858  accuracy:  0.216675\n",
      "epoch:  342  loss:  0.21286509931087494  accuracy:  0.216675\n",
      "epoch:  343  loss:  0.21273528039455414  accuracy:  0.216675\n",
      "epoch:  344  loss:  0.21260537207126617  accuracy:  0.216675\n",
      "epoch:  345  loss:  0.21247538924217224  accuracy:  0.216675\n",
      "epoch:  346  loss:  0.21234531700611115  accuracy:  0.216675\n",
      "epoch:  347  loss:  0.21221515536308289  accuracy:  0.216675\n",
      "epoch:  348  loss:  0.21208490431308746  accuracy:  0.216675\n",
      "epoch:  349  loss:  0.21195456385612488  accuracy:  0.216675\n",
      "epoch:  350  loss:  0.21182413399219513  accuracy:  0.216675\n",
      "epoch:  351  loss:  0.2116936296224594  accuracy:  0.216675\n",
      "epoch:  352  loss:  0.21156302094459534  accuracy:  0.216675\n",
      "epoch:  353  loss:  0.2114323228597641  accuracy:  0.216675\n",
      "epoch:  354  loss:  0.21130156517028809  accuracy:  0.216675\n",
      "epoch:  355  loss:  0.21117070317268372  accuracy:  0.216675\n",
      "epoch:  356  loss:  0.2110397219657898  accuracy:  0.216675\n",
      "epoch:  357  loss:  0.2109086662530899  accuracy:  0.216675\n",
      "epoch:  358  loss:  0.21077750623226166  accuracy:  0.216675\n",
      "epoch:  359  loss:  0.21064625680446625  accuracy:  0.216675\n",
      "epoch:  360  loss:  0.21051490306854248  accuracy:  0.216675\n",
      "epoch:  361  loss:  0.21038347482681274  accuracy:  0.216675\n",
      "epoch:  362  loss:  0.21025191247463226  accuracy:  0.216675\n",
      "epoch:  363  loss:  0.210120290517807  accuracy:  0.216675\n",
      "epoch:  364  loss:  0.2099885493516922  accuracy:  0.216675\n",
      "epoch:  365  loss:  0.20985673367977142  accuracy:  0.216675\n",
      "epoch:  366  loss:  0.2097247987985611  accuracy:  0.216675\n",
      "epoch:  367  loss:  0.2095927894115448  accuracy:  0.216675\n",
      "epoch:  368  loss:  0.20946069061756134  accuracy:  0.216675\n",
      "epoch:  369  loss:  0.20932844281196594  accuracy:  0.216675\n",
      "epoch:  370  loss:  0.20919613540172577  accuracy:  0.216675\n",
      "epoch:  371  loss:  0.20906370878219604  accuracy:  0.216675\n",
      "epoch:  372  loss:  0.20893119275569916  accuracy:  0.216675\n",
      "epoch:  373  loss:  0.2087985873222351  accuracy:  0.216675\n",
      "epoch:  374  loss:  0.2086658775806427  accuracy:  0.216675\n",
      "epoch:  375  loss:  0.20853304862976074  accuracy:  0.216675\n",
      "epoch:  376  loss:  0.20840013027191162  accuracy:  0.216675\n",
      "epoch:  377  loss:  0.20826710760593414  accuracy:  0.216675\n",
      "epoch:  378  loss:  0.2081340104341507  accuracy:  0.216675\n",
      "epoch:  379  loss:  0.2080007791519165  accuracy:  0.216675\n",
      "epoch:  380  loss:  0.20786745846271515  accuracy:  0.216675\n",
      "epoch:  381  loss:  0.20773401856422424  accuracy:  0.216675\n",
      "epoch:  382  loss:  0.20760047435760498  accuracy:  0.216675\n",
      "epoch:  383  loss:  0.20746685564517975  accuracy:  0.216675\n",
      "epoch:  384  loss:  0.20733310282230377  accuracy:  0.216675\n",
      "epoch:  385  loss:  0.20719929039478302  accuracy:  0.216675\n",
      "epoch:  386  loss:  0.20706535875797272  accuracy:  0.216675\n",
      "epoch:  387  loss:  0.20693130791187286  accuracy:  0.216675\n",
      "epoch:  388  loss:  0.20679715275764465  accuracy:  0.216675\n",
      "epoch:  389  loss:  0.20666290819644928  accuracy:  0.216675\n",
      "epoch:  390  loss:  0.20652855932712555  accuracy:  0.216675\n",
      "epoch:  391  loss:  0.20639410614967346  accuracy:  0.216675\n",
      "epoch:  392  loss:  0.20625951886177063  accuracy:  0.216675\n",
      "epoch:  393  loss:  0.20612484216690063  accuracy:  0.216675\n",
      "epoch:  394  loss:  0.20599007606506348  accuracy:  0.216675\n",
      "epoch:  395  loss:  0.20585519075393677  accuracy:  0.216675\n",
      "epoch:  396  loss:  0.2057202011346817  accuracy:  0.216675\n",
      "epoch:  397  loss:  0.20558512210845947  accuracy:  0.216675\n",
      "epoch:  398  loss:  0.2054499238729477  accuracy:  0.216675\n",
      "epoch:  399  loss:  0.20531460642814636  accuracy:  0.216675\n",
      "epoch:  400  loss:  0.20517918467521667  accuracy:  0.216675\n",
      "epoch:  401  loss:  0.20504367351531982  accuracy:  0.216675\n",
      "epoch:  402  loss:  0.20490802824497223  accuracy:  0.216675\n",
      "epoch:  403  loss:  0.20477226376533508  accuracy:  0.216675\n",
      "epoch:  404  loss:  0.20463643968105316  accuracy:  0.216675\n",
      "epoch:  405  loss:  0.2045004665851593  accuracy:  0.216675\n",
      "epoch:  406  loss:  0.20436441898345947  accuracy:  0.216675\n",
      "epoch:  407  loss:  0.2042282074689865  accuracy:  0.216675\n",
      "epoch:  408  loss:  0.20409192144870758  accuracy:  0.216675\n",
      "epoch:  409  loss:  0.2039555013179779  accuracy:  0.216675\n",
      "epoch:  410  loss:  0.20381899178028107  accuracy:  0.216675\n",
      "epoch:  411  loss:  0.20368236303329468  accuracy:  0.216675\n",
      "epoch:  412  loss:  0.20354562997817993  accuracy:  0.216675\n",
      "epoch:  413  loss:  0.20340876281261444  accuracy:  0.216675\n",
      "epoch:  414  loss:  0.2032717764377594  accuracy:  0.216675\n",
      "epoch:  415  loss:  0.2031347006559372  accuracy:  0.216675\n",
      "epoch:  416  loss:  0.20299750566482544  accuracy:  0.216675\n",
      "epoch:  417  loss:  0.20286019146442413  accuracy:  0.216675\n",
      "epoch:  418  loss:  0.20272275805473328  accuracy:  0.216675\n",
      "epoch:  419  loss:  0.20258519053459167  accuracy:  0.216675\n",
      "epoch:  420  loss:  0.2024475336074829  accuracy:  0.216675\n",
      "epoch:  421  loss:  0.2023097723722458  accuracy:  0.216675\n",
      "epoch:  422  loss:  0.20217187702655792  accuracy:  0.216675\n",
      "epoch:  423  loss:  0.2020338624715805  accuracy:  0.216675\n",
      "epoch:  424  loss:  0.20189572870731354  accuracy:  0.216675\n",
      "epoch:  425  loss:  0.20175747573375702  accuracy:  0.216675\n",
      "epoch:  426  loss:  0.20161910355091095  accuracy:  0.216675\n",
      "epoch:  427  loss:  0.20148061215877533  accuracy:  0.216675\n",
      "epoch:  428  loss:  0.20134201645851135  accuracy:  0.216675\n",
      "epoch:  429  loss:  0.20120325684547424  accuracy:  0.216675\n",
      "epoch:  430  loss:  0.20106440782546997  accuracy:  0.216675\n",
      "epoch:  431  loss:  0.20092542469501495  accuracy:  0.216675\n",
      "epoch:  432  loss:  0.20078632235527039  accuracy:  0.216675\n",
      "epoch:  433  loss:  0.20064711570739746  accuracy:  0.216675\n",
      "epoch:  434  loss:  0.2005077302455902  accuracy:  0.216675\n",
      "epoch:  435  loss:  0.20036828517913818  accuracy:  0.216675\n",
      "epoch:  436  loss:  0.20022867619991302  accuracy:  0.216675\n",
      "epoch:  437  loss:  0.20008894801139832  accuracy:  0.216675\n",
      "epoch:  438  loss:  0.19994910061359406  accuracy:  0.216675\n",
      "epoch:  439  loss:  0.19980911910533905  accuracy:  0.216675\n",
      "epoch:  440  loss:  0.1996690183877945  accuracy:  0.216675\n",
      "epoch:  441  loss:  0.1995287835597992  accuracy:  0.216675\n",
      "epoch:  442  loss:  0.19938844442367554  accuracy:  0.216675\n",
      "epoch:  443  loss:  0.19924792647361755  accuracy:  0.216675\n",
      "epoch:  444  loss:  0.1991073191165924  accuracy:  0.216675\n",
      "epoch:  445  loss:  0.19896656274795532  accuracy:  0.216675\n",
      "epoch:  446  loss:  0.19882570207118988  accuracy:  0.216675\n",
      "epoch:  447  loss:  0.1986846625804901  accuracy:  0.216675\n",
      "epoch:  448  loss:  0.19854354858398438  accuracy:  0.216675\n",
      "epoch:  449  loss:  0.1984022855758667  accuracy:  0.216675\n",
      "epoch:  450  loss:  0.19826087355613708  accuracy:  0.216675\n",
      "epoch:  451  loss:  0.19811934232711792  accuracy:  0.216675\n",
      "epoch:  452  loss:  0.19797766208648682  accuracy:  0.216675\n",
      "epoch:  453  loss:  0.19783586263656616  accuracy:  0.216675\n",
      "epoch:  454  loss:  0.19769389927387238  accuracy:  0.216675\n",
      "epoch:  455  loss:  0.19755186140537262  accuracy:  0.216675\n",
      "epoch:  456  loss:  0.19740964472293854  accuracy:  0.216675\n",
      "epoch:  457  loss:  0.19726727902889252  accuracy:  0.216675\n",
      "epoch:  458  loss:  0.19712480902671814  accuracy:  0.216675\n",
      "epoch:  459  loss:  0.19698219001293182  accuracy:  0.216675\n",
      "epoch:  460  loss:  0.19683943688869476  accuracy:  0.216675\n",
      "epoch:  461  loss:  0.19669651985168457  accuracy:  0.216675\n",
      "epoch:  462  loss:  0.19655349850654602  accuracy:  0.216675\n",
      "epoch:  463  loss:  0.19641032814979553  accuracy:  0.216675\n",
      "epoch:  464  loss:  0.1962670087814331  accuracy:  0.216675\n",
      "epoch:  465  loss:  0.19612355530261993  accuracy:  0.216675\n",
      "epoch:  466  loss:  0.19597996771335602  accuracy:  0.216675\n",
      "epoch:  467  loss:  0.19583621621131897  accuracy:  0.216675\n",
      "epoch:  468  loss:  0.19569234549999237  accuracy:  0.216675\n",
      "epoch:  469  loss:  0.19554832577705383  accuracy:  0.216675\n",
      "epoch:  470  loss:  0.19540415704250336  accuracy:  0.216675\n",
      "epoch:  471  loss:  0.19525983929634094  accuracy:  0.216675\n",
      "epoch:  472  loss:  0.19511538743972778  accuracy:  0.216675\n",
      "epoch:  473  loss:  0.19497078657150269  accuracy:  0.216675\n",
      "epoch:  474  loss:  0.19482605159282684  accuracy:  0.216675\n",
      "epoch:  475  loss:  0.19468115270137787  accuracy:  0.216675\n",
      "epoch:  476  loss:  0.19453613460063934  accuracy:  0.216675\n",
      "epoch:  477  loss:  0.1943909376859665  accuracy:  0.216675\n",
      "epoch:  478  loss:  0.1942456215620041  accuracy:  0.216675\n",
      "epoch:  479  loss:  0.19410014152526855  accuracy:  0.216675\n",
      "epoch:  480  loss:  0.19395451247692108  accuracy:  0.216675\n",
      "epoch:  481  loss:  0.19380873441696167  accuracy:  0.216675\n",
      "epoch:  482  loss:  0.19366279244422913  accuracy:  0.216675\n",
      "epoch:  483  loss:  0.19351671636104584  accuracy:  0.216675\n",
      "epoch:  484  loss:  0.1933705061674118  accuracy:  0.216675\n",
      "epoch:  485  loss:  0.19322414696216583  accuracy:  0.216675\n",
      "epoch:  486  loss:  0.19307762384414673  accuracy:  0.216675\n",
      "epoch:  487  loss:  0.1929309219121933  accuracy:  0.216675\n",
      "epoch:  488  loss:  0.19278408586978912  accuracy:  0.216675\n",
      "epoch:  489  loss:  0.19263708591461182  accuracy:  0.216675\n",
      "epoch:  490  loss:  0.19248996675014496  accuracy:  0.216675\n",
      "epoch:  491  loss:  0.19234265387058258  accuracy:  0.216675\n",
      "epoch:  492  loss:  0.19219523668289185  accuracy:  0.216675\n",
      "epoch:  493  loss:  0.1920476108789444  accuracy:  0.216675\n",
      "epoch:  494  loss:  0.1918998509645462  accuracy:  0.216675\n",
      "epoch:  495  loss:  0.19175192713737488  accuracy:  0.216675\n",
      "epoch:  496  loss:  0.1916038691997528  accuracy:  0.216675\n",
      "epoch:  497  loss:  0.1914556324481964  accuracy:  0.216675\n",
      "epoch:  498  loss:  0.19130724668502808  accuracy:  0.216675\n",
      "epoch:  499  loss:  0.1911586970090866  accuracy:  0.216675\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    epoch_list.append(epoch)\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x.float())\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    ct, ttl_ct = 0, 0\n",
    "\n",
    "    y_pred_temp = y_pred.detach().numpy()\n",
    "    y_temp = y.detach().numpy()\n",
    "    for i in range(len(y_pred_temp)):\n",
    "        ct += (np.argmax(y_pred_temp[i]) == np.argmax(y_temp[i]))\n",
    "        ttl_ct += 1\n",
    "\n",
    "    acc = ct / ttl_ct\n",
    "    acc_list.append(acc)\n",
    "\n",
    "    emos_pred = model(torch.tensor(test_tokens).float())\n",
    "    emos_pred = emos_pred.detach().numpy()\n",
    "\n",
    "    ct, ttl_ct = 0, 0\n",
    "\n",
    "    for i in range(len(emos_pred)):\n",
    "        ct += (np.argmax(emos_pred[i]) == np.argmax(test_emos[i]))\n",
    "        ttl_ct += 1\n",
    "\n",
    "    t_acc = ct / ttl_ct\n",
    "    acc_list_test.append(t_acc)\n",
    "\n",
    "    print('epoch: ', epoch, ' loss: ', loss.item(), ' accuracy: ', acc)\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    # perform a backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "    # Update the parameters\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "emos_pred = model(torch.tensor(test_tokens).float())\n",
    "emos_pred = emos_pred.detach().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_list, acc_list, color='skyblue', label='test')\n",
    "plt.plot(epoch_list, acc_list_test, color='green', label='train')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "0.20932093209320932"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct, ttl_ct = 0, 0\n",
    "\n",
    "for i in range(len(emos_pred)):\n",
    "    ct += (np.argmax(emos_pred[i]) == np.argmax(test_emos[i]))\n",
    "    ttl_ct += 1\n",
    "\n",
    "ct / ttl_ct"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'nn_BoW.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}